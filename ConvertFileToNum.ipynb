{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtbP5U4mtd0yMPAY2Vcznz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oyinig/African-AI-Foundation/blob/main/ConvertFileToNum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjoJCpG-Mo5k",
        "outputId": "2b5c12f9-8af6-4368-83c2-d6e4c07269dc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.33.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hXmLWrzG1Y3M"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "import shutil\n",
        "import os\n",
        "import io\n",
        "from transformers import Wav2Vec2FeatureExtractor, AutoTokenizer\n",
        "import torchaudio\n",
        "from torchaudio.transforms import Resample\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert an image in the content folder to numbers"
      ],
      "metadata": {
        "id": "ondWhC6CLjpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def image_to_matrix():\n",
        "    image = Image.open('/content/sample_data/Images/image.jpeg')\n",
        "    # Convert the image to grayscale\n",
        "    image_gray = image.convert('L')\n",
        "    # Convert the grayscale image to a numpy array\n",
        "    matrix_grey = np.array(image_gray)\n",
        "    print(\"Grey\", matrix_grey)\n",
        "    matrix_coloured = np.array(image)\n",
        "    print(\"Coloured\", matrix_coloured)\n",
        "    print(\"Grey Shape\", matrix_grey.shape)\n",
        "    print(\"Coloured Shape\", matrix_coloured.shape)\n",
        "    np.savetxt(\"/content/image_data.csv\", matrix_grey)\n",
        "\n",
        "\n",
        "image_to_matrix()"
      ],
      "metadata": {
        "id": "b4Fj8CR81era",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1493f09-7f51-42c1-b0b4-0a4fa980f0d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grey [[255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " ...\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]]\n",
            "Coloured [[[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]]\n",
            "Grey Shape (213, 237)\n",
            "Coloured Shape (213, 237, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_audio_features():\n",
        "    audio_file = '/content/sample_data/Audio/sample-3s.mp3'\n",
        "\n",
        "    waveform, original_sample_rate = torchaudio.load(audio_file)\n",
        "\n",
        "    # Resample the audio waveform to 16000 Hz\n",
        "    resample_transform = Resample(orig_freq=original_sample_rate, new_freq=16000)\n",
        "    resampled_waveform = resample_transform(waveform)\n",
        "\n",
        "    # Initialize Wav2Vec2 feature extractor\n",
        "    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "    # Extract audio features\n",
        "    features = feature_extractor(resampled_waveform, sampling_rate=16000, return_tensors=\"pt\")\n",
        "    np.save('/content/extracted_audio_features.npy', features.input_values.numpy())\n",
        "    print(features)\n",
        "    print(features.input_values.shape)\n",
        "\n",
        "extract_audio_features()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1125UlrVKB9n",
        "outputId": "5efd728b-367f-42f3-8d31-6183f3af403e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_values': tensor([[[-0.2358,  0.0660,  0.2489,  ..., -0.2437,  0.0678,  0.2143],\n",
            "         [-0.2358,  0.0660,  0.2489,  ..., -0.2437,  0.0678,  0.2143]]])}\n",
            "torch.Size([1, 2, 51131])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Input words - Try to make them the same no of words\n",
        "input_words1 = \"Convert these words to numbers. African AI Foundation is creating an army of AI champions\"\n",
        "input_words2 = \"Converted the words to numbers. African AI Foundation has created an army of AI champions\"\n",
        "input_words3 =  \"The African economy needs to grow for it to become very competitive like the European economy\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Tokenize the input words\n",
        "tokens1 = tokenizer(input_words1, return_tensors=\"pt\")\n",
        "tokens2 = tokenizer(input_words2, return_tensors=\"pt\")\n",
        "tokens3 = tokenizer(input_words3, return_tensors=\"pt\")\n",
        "\n",
        "# Convert tokens to numbers (token IDs)\n",
        "token_ids = tokens['input_ids']\n",
        ""
      ],
      "metadata": {
        "id": "36MKw4b3HmxN"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cosine Similarity search between token 1 and 2. Higher number means very similar\n",
        "\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "cos_sim = dot(tokens1.input_ids[0], tokens2.input_ids[0])/(norm(tokens1.input_ids[0])*norm(tokens2.input_ids[0]))\n",
        "cos_sim"
      ],
      "metadata": {
        "id": "mKR0vSSQfR3M",
        "outputId": "1786f0b4-0501-49db-b037-5a90433a992a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9622583131501405"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cosine Similarity search between token 2 and 3. Lower number means less similar\n",
        "\n",
        "cos_sim = dot(tokens2.input_ids[0], tokens3.input_ids[0])/(norm(tokens2.input_ids[0])*norm(tokens3.input_ids[0]))\n",
        "cos_sim"
      ],
      "metadata": {
        "id": "Hw8bzgUVVJLG",
        "outputId": "4e4cceb1-c193-4c07-8ab7-42591be7e74d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6601614588771935"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2wazvA1qLhy-"
      }
    }
  ]
}